{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "8dd9b572c805487a9fb430fdc4ab12bb",
    "deepnote_cell_height": 156.26666259765625,
    "deepnote_cell_type": "markdown",
    "id": "XUZ1dFPHzAHl"
   },
   "source": [
    "<h1><center>Laboratorio 3: La desperaci√≥n de Mr. Cheems üêº</center></h1>\n",
    "\n",
    "<center><strong>MDS7202: Laboratorio de Programaci√≥n Cient√≠fica para Ciencia de Datos - Oto√±o 2025</strong></center>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "d65413cd8566460dbceffcd13ca236e7",
    "deepnote_cell_type": "markdown",
    "id": "UD8X1uhGzAHq"
   },
   "source": [
    "### Cuerpo Docente:\n",
    "\n",
    "- Profesores: Stefano Schiappacasse, Sebasti√°n Tinoco\n",
    "- Auxiliares: Melanie Pe√±a, Valentina Rojas\n",
    "- Ayudantes: Angelo Mu√±oz, Valentina Z√∫√±iga"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "8e9217d02d124830a9b86046600a1605",
    "deepnote_cell_height": 172.13333129882812,
    "deepnote_cell_type": "markdown",
    "id": "tXflExjqzAHr"
   },
   "source": [
    "### Equipo: SUPER IMPORTANTE - notebooks sin nombre no ser√°n revisados\n",
    "\n",
    "- Nombre de alumno 1:   Sasha Oyanadel\n",
    "- Nombre de alumno 2:   Pablo Vergara\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "010402b6d5f743b885a80d2e1c6ae11a",
    "deepnote_cell_height": 62.19999694824219,
    "deepnote_cell_type": "markdown",
    "id": "AD-V0bbZzAHr"
   },
   "source": [
    "### **Link de repositorio de GitHub:** [Repositorio](https://github.com/pablo30vll/Laboratorio3_MDS7202-)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "ef0224c7a99e4b718b55493b0a1e99c4",
    "deepnote_cell_height": 724.9000244140625,
    "deepnote_cell_type": "markdown",
    "id": "6uBLPj1PzAHs"
   },
   "source": [
    "## Temas a tratar\n",
    "- Aplicar Pandas para obtener caracter√≠sticas de un DataFrame.\n",
    "- Aplicar Pipelines y Column Transformers\n",
    "\n",
    "## Reglas:\n",
    "\n",
    "- **Grupos de 2 personas**\n",
    "- Fecha de entrega: 6 d√≠as de plazo con descuento de 1 punto por d√≠a. Entregas Martes a las 23:59.\n",
    "- Instrucciones del lab el viernes a las 16:15 en formato online. Asistencia no es obligatoria, pero se recomienda fuertemente asistir.\n",
    "- <u>Prohibidas las copias</u>. Cualquier intento de copia ser√° debidamente penalizado con el reglamento de la escuela.\n",
    "- Tienen que subir el laboratorio a u-cursos y a su repositorio de github. Labs que no est√©n en u-cursos no ser√°n revisados. Recuerden que el repositorio tambi√©n tiene nota.\n",
    "- Cualquier duda fuera del horario de clases al foro. Mensajes al equipo docente ser√°n respondidos por este medio.\n",
    "- Pueden usar cualquier material del curso que estimen conveniente.\n",
    "\n",
    "### Objetivos principales del laboratorio\n",
    "- Comprender c√≥mo aplicar pipelines de Scikit-Learn para generar procesos m√°s limpios en Feature Engineering.\n",
    "\n",
    "El laboratorio deber√° ser desarrollado sin el uso indiscriminado de iteradores nativos de python (aka \"for\", \"while\"). La idea es que aprendan a exprimir al m√°ximo las funciones optimizadas que nos entrega `numpy`, las cuales vale mencionar, son bastante m√°s eficientes que los iteradores nativos sobre arreglos (*o tensores*)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "59664481c26f4ac4a753765269b1db6a",
    "deepnote_cell_height": 69.86666870117188,
    "deepnote_cell_type": "markdown",
    "id": "wrG4gYabzAHs"
   },
   "source": [
    "## Descripci√≥n del laboratorio."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "8c7bf8ea553d44c7a2efd61106a0bac2",
    "deepnote_cell_height": 61.866668701171875,
    "deepnote_cell_type": "markdown",
    "id": "MhISwri4zAHy"
   },
   "source": [
    "### Importamos librerias utiles üò∏"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "67b4b29f0e6b48719b58d579276f2b19",
    "deepnote_cell_height": 514.13330078125,
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 8517,
    "execution_start": 1635469788590,
    "id": "uyc33dKdzAHy",
    "source_hash": "a3741fd5"
   },
   "outputs": [],
   "source": [
    "# Libreria Core del lab.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from IPython.display import HTML\n",
    "\n",
    "# Libreria para plotear (En colab esta desactualizado plotly)\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Librerias utiles\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import FunctionTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "ce6a19ec6fc6486e832760ac3740d7ef",
    "deepnote_cell_height": 219.46665954589844,
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 7,
    "execution_start": 1635165625274,
    "id": "gQ0-zPV4NNrq",
    "outputId": "a7c33afa-37fe-4965-de1a-53b8994c8c07",
    "source_hash": "c60dc4a7"
   },
   "outputs": [],
   "source": [
    "# Si usted est√° utilizando Colabolatory le puede ser √∫til este c√≥digo para cargar los archivos.\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount(\"/content/drive\")\n",
    "    path = 'Direcci√≥n donde tiene los archivos en el Drive'\n",
    "except:\n",
    "    print('Ignorando conexi√≥n drive-colab')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "28c7a8b483d84878ac5a4f7ba882b711",
    "deepnote_cell_height": 133.86666870117188,
    "deepnote_cell_type": "markdown",
    "id": "QDwIXTh7bK_A",
    "owner_user_id": "badcc427-fd3d-4615-9296-faa43ec69cfb"
   },
   "source": [
    "# Feature engineering en datos de retail üõçÔ∏è"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "160bb2695f6547448bfb0f99420f952c",
    "deepnote_cell_height": 69.86666870117188,
    "deepnote_cell_type": "markdown",
    "id": "_Eu4qBqnXMff",
    "tags": []
   },
   "source": [
    "### 0. Cargar Dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "6c6799ecc9e74272922d46a3b5a8b79e",
    "deepnote_cell_height": 294.683349609375,
    "deepnote_cell_type": "markdown",
    "id": "4shIzqqwXMfe",
    "tags": []
   },
   "source": [
    "<p align=\"center\">\n",
    "  <img width=300 src=\"https://s1.eestatic.com/2018/04/14/social/la_jungla_-_social_299733421_73842361_854x640.jpg\">\n",
    "</p>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "48d29c89e3b6455083f8fac764f97f3b",
    "deepnote_cell_height": 475.066650390625,
    "deepnote_cell_type": "markdown",
    "id": "cDpKjYRCXMfg",
    "tags": []
   },
   "source": [
    "Mr. Cheems, gerente de una cotizada tienda de retail en Europa, les solicita si pueden analizar los datos de algunas de sus tiendas. En una reuni√≥n, Mr Cheems le comenta que la calidad de sus datos no es muy buena, por lo que le solicita a usted que limpie su base de datos y cree nuevos atributos relevantes para el negocio.\n",
    "\n",
    "Por ello, el √°rea de ventas les entrega archivo llamado `online_retail_data.pickle` el cual usted decide cargar a continuaci√≥n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "4d7d0f0855744e6c9d5a2198e5dcd690",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "deepnote_cell_height": 489.79998779296875,
    "deepnote_cell_type": "code",
    "deepnote_output_heights": [
     177
    ],
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 466,
    "execution_start": 1635469797118,
    "id": "7FNOu-CvjV5m",
    "outputId": "90b4f92c-71df-44d4-8084-4dd06a6179e4",
    "source_hash": "d52b246c"
   },
   "outputs": [],
   "source": [
    "# Inserte su c√≥digo aqu√≠\n",
    "df_retail = pd.read_pickle(\"/Users/locoplaya666/Desktop/labslaboratoriodatos/lab3/online_retail_data.pickle\")\n",
    "df_retail.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "Q6nm_0uWvrFv"
   },
   "source": [
    "### 1. Funci√≥n para explorar caracter√≠sticas [0.5 puntos]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "KOZEZbbLoqfI"
   },
   "source": [
    "<p align=\"center\">\n",
    "  <img width=300 src=\"https://editor.analyticsvidhya.com/uploads/47389meme.png\">\n",
    "</p>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "J-7ZaNutk2GO"
   },
   "source": [
    "\n",
    "\n",
    "Tras inspeccionar brevemente los datos proporcionados, usted decide crear una funci√≥n que realice lo siguiente:\n",
    "- Plotee un histograma para las variables precios y cantidad. [0.3 puntos]\n",
    "- Imprima un conteo de datos nulos por variable [0.2 puntos]\n",
    "\n",
    "**Nota**: Para generar los gr√°ficos no es obligatorio el uso de `plotly`, pero si es altamente recomendado. Pueden encontrar m√°s informaci√≥n de esta librer√≠a en este [enlace](https://plotly.com/python/)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "TM8FZ_4Yuiwi"
   },
   "source": [
    "**Respuesta:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly\n",
    "import numpy as np\n",
    "\n",
    "print(\"Plotly:\", plotly.__version__)\n",
    "print(\"Numpy:\", np.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uDqT1Ljpk7vp"
   },
   "outputs": [],
   "source": [
    "def explore_data(df, nulls=False, columns=[\"Price\", \"Quantity\"]):\n",
    "    for col in columns:\n",
    "        max_val = df[col].max()\n",
    "        fig = px.histogram(df, x=col, nbins=35, title=f\"Histograma de {col}\", range_x=[0, max_val])\n",
    "        fig.show()\n",
    "\n",
    "    if nulls:\n",
    "        print(\"Conteo de datos nulos por variable:\")\n",
    "        print(df[columns].isnull().sum())\n",
    "\n",
    "explore_data(df_retail,nulls=True)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "F4ZY_N0Ad1GP"
   },
   "source": [
    "### 2. Eliminando outliers [1.0 puntos]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "yXTpIi1Bo2KG"
   },
   "source": [
    "<p align=\"center\">\n",
    "  <img width=300 src=\"https://media.licdn.com/dms/image/C5612AQGdXKCka7HumA/article-cover_image-shrink_600_2000/0/1520056407281?e=2147483647&v=beta&t=VZcfjjzjK4LxXdZkSu1KisWC0Ry8bk4tPCn3R8aYdNM\">\n",
    "</p>\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "ECqH4t-Jvj05"
   },
   "source": [
    "#### 2.1 Creando la clase IQR [0.5 puntos]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "GtCQGHN_mzEp"
   },
   "source": [
    "Entre las falencias de los datos, Mr. Cheems le comenta que a veces los operadores no ingresan el precio correcto de los productos. Mr. Cheems le comenta que se dio cuenta de este fen√≥meno porque hay productos con precios exager√°damente altos o bajos. Por lo cual usted decide eliminar outliers del dataframe a traves del rango intercuartil el cual cuenta con los siguientes pasos:\n",
    "\n",
    "1. Calcular el primer cuartil $Q1$ y el tercer cuartil $Q3$. Hint: utilice el m√©todo `quantile()`\n",
    "\n",
    "2. Calcular el rango intercuartil (RIC): $RIC = Q3 - Q1$\n",
    "\n",
    "3. Calcular los l√≠mites para identificar outliers:\n",
    " - L√≠mite inferior: $~~Q1 - \\lambda \\cdot RIC$\n",
    " - L√≠mite superior: $~~Q3 + \\lambda \\cdot RIC$\n",
    "\n",
    "4. Eliminar outliers: Los outliers son los datos que est√°n por debajo del l√≠mite inferior o por encima del l√≠mite superior.\n",
    "\n",
    "\n",
    "Para realizar dicha tarea, usted decide crear una clase llamada `IQR()` utilizando `BaseEstimator` y `TransformerMixin` para realizar una transformaci√≥n de cada una de las columnas num√©ricas del DataFrame utilizando `ColumnTransformer()` m√°s tarde. Considere que lambda debe ser $\\lambda$ un par√°metro a definir por el usuario.\n",
    "\n",
    "**Hint:** tome como referencia el siguiente [enlace](https://sklearn-template.readthedocs.io/en/latest/user_guide.html#transformer).\n",
    "\n",
    "**Nota:** No modificar el m√©todo set_output de la clase IQR"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "3uqK6AZnuhmL"
   },
   "source": [
    "**Respuesta:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "70CGFkRScKKP"
   },
   "outputs": [],
   "source": [
    "class IQR(BaseEstimator, TransformerMixin):\n",
    "\n",
    "  def __init__(self, lam = 1.5):\n",
    "    self.lam = lam\n",
    "    self.q1 = None\n",
    "    self.q2 = None\n",
    "    self.q3 = None\n",
    "    self.ric = None\n",
    "\n",
    "  def fit(self, X):\n",
    "    X= pd.DataFrame(X)\n",
    "    self.q1 = X.quantile(0.25)\n",
    "    self.q3 = X.quantile(0.75)\n",
    "    self.ric = self.q3-self.q3\n",
    "    return self\n",
    "\n",
    "  def transform(self, X):\n",
    "    X= pd.DataFrame(X)\n",
    "    lower_bound = self.q1-self.ric*self.lam\n",
    "    upper_bound = self.q3+self.ric*self.lam\n",
    "    mask_list = []\n",
    "\n",
    "    for idx, row in X.iterrows():\n",
    "        es_valida = True\n",
    "        for col in X.columns:\n",
    "            if row[col] < lower_bound[col] or row[col] > upper_bound[col]:\n",
    "                es_valida = False\n",
    "                break  \n",
    "        mask_list.append(es_valida)\n",
    "\n",
    "    return X[mask_list]\n",
    "\n",
    "\n",
    "  def set_output(self,transform='default'):\n",
    "    #No modificar esta funci√≥n\n",
    "    return self"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "Pse94ohOm1um"
   },
   "source": [
    "#### 2.2 Creaci√≥n del Pipeline [0.5 puntos]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "JVWWiGA5m_Hj"
   },
   "source": [
    "Para comenzar introduci√©ndose en el uso de pipeline, usted decide definir un pipeline con el Transformer previamente definido. Adem√°s, usted decide visualizar c√≥mo cambia la distribuci√≥n de las variables Precio y Cantidad antes y despues de aplicar IQR. Para ello, usted aplica los siguientes pasos:\n",
    "\n",
    "- Definir un pipeline llamado `numeric_transformations` para las variables precio y cantidad con la transformaci√≥n IQR. [0.1 puntos]\n",
    "- Defina un column transformer que aplique `numeric_transformations` para las variables num√©ricas y `passthrough` para las variables categ√≥ricas. Adicionalmente, fije el par√°metro `verbose_feature_names_out` en `False`. Ver hint al final [0.1 puntos]\n",
    "- Defina el dataframe `df_iqr` aplicado el column transformer a los datos proporcionados por Mr. Cheems considerando un valor de $\\lambda$ que tenga un desempe√±o aceptable para ambas variables. [0.1 puntos]\n",
    "- Usar `explore_data` en `df_retail` y en `df_iqr`.  [0.1 puntos]\n",
    "- Reportar los cambios observados en la distribuci√≥n de las variables. ¬øQu√© sucede al aumentar el valor de lambda? [0.1 puntos]\n",
    "\n",
    "\n",
    "**Hint:** El transformador `passthrough` est√° predefinido y es una opci√≥n que puedes usar para las columnas que no deseas transformar. Al especificar 'passthrough' para una parte de tu ColumnTransformer, las columnas correspondientes pasar√°n a trav√©s del ColumnTransformer sin ninguna modificaci√≥n. El siguiente [enlace](https://scikit-learn.org/stable/modules/generated/sklearn.compose.ColumnTransformer.html) le puede ser √∫til.\n",
    "\n",
    "**Nota:** Mantenga el m√©todo set_output del column transformer con la transformaci√≥n `pandas` para obtener un dataframe una vez aplicado el column transformer."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "vkeizZcLuabD"
   },
   "source": [
    "**Respuesta:**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "LF24vWb4GwLo"
   },
   "source": [
    "Ap√≥yese de la siguiente estructura para su respuesta:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZaSuz2NSn7g6"
   },
   "outputs": [],
   "source": [
    "valores_lambda = [0.5, 1, 1.5, 2]\n",
    "\n",
    "numerical_columns = [\"Price\", \"Quantity\"]\n",
    "categorical_columns = [\"Invoice\", \"StockCode\", \"Description\", \"InvoiceDate\", \"Customer ID\", \"Country\"]\n",
    "\n",
    "def explorar_con_lambda(lam):\n",
    "    print(f\"\\n================ Œª = {lam} ================\\n\")\n",
    "\n",
    "    numeric_transformations = Pipeline([\n",
    "        (\"iqr_filter\", IQR(lam=lam))\n",
    "    ])\n",
    "\n",
    "    column_transformer = ColumnTransformer([\n",
    "        (\"numerical\", numeric_transformations, numerical_columns),\n",
    "        (\"categorical\", \"passthrough\", categorical_columns)\n",
    "    ], verbose_feature_names_out=False)\n",
    "\n",
    "    column_transformer.set_output(transform=\"pandas\")\n",
    "\n",
    "    df_iqr = column_transformer.fit_transform(df_retail)\n",
    "\n",
    "\n",
    "    print(\"\\nDistribuci√≥n de Price y Quantity con outliers eliminados:\\n\")\n",
    "    explore_data(df_iqr)\n",
    "\n",
    "for lam in valores_lambda:\n",
    "    explorar_con_lambda(lam)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "yPKnc6UcsDkm"
   },
   "source": [
    "Al aplicar el filtro de outliers utilizando el m√©todo del rango intercuartil (IQR), se observa un cambio significativo en la distribuci√≥n de las variables Price y Quantity. En los histogramas originales, sin filtrar, ambas variables presentan valores extremadamente altos que aplastan la visualizaci√≥n, haciendo que la mayor parte de los datos se concentren cerca de cero y ocultando su distribuci√≥n real. Al aplicar el filtro IQR con distintos valores de lambda (Œª = 0.5, 1, 1.5 y 2), los histogramas cambian dr√°sticamente, mostrando una distribuci√≥n mucho m√°s clara y detallada, donde se identifican frecuencias t√≠picas de precio y cantidad. Sin embargo, se observa que al variar lambda en ese rango, los gr√°ficos resultantes no cambian significativamente. Esto ocurre porque los outliers en este dataset son valores extremadamente lejanos del centro de la distribuci√≥n, por lo que quedan fuera del rango permitido incluso con valores de Œª relativamente altos."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "MF5s4dqMYCbJ"
   },
   "source": [
    "### 3. Agregando un imputer al pipeline [1.0 puntos]\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "2Bc9fFeXp-At"
   },
   "source": [
    "<p align=\"center\">\n",
    "  <img width=300 src=\"https://media.makeameme.org/created/hmm-there-is.jpg\">\n",
    "</p>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "uugEdc26vJ5N"
   },
   "source": [
    "Para continuar con la limpieza del dataframe usted decide imputar los datos nulos de las variables num√©ricas, para lo cual decide realizar las siguientes tareas:\n",
    "\n",
    "1. Crear un pipeline para variables categ√≥ricas llamado `categoric_transformations` con un paso llamado `mode_imputer`, en el cual se imputen los datos faltantes por la categor√≠a m√°s frecuente.\n",
    "2. Agregar al pipeline `numeric_transformations` un paso llamado `mean_imputer`, en el cual se imputen los datos por la media usando [SimpleImputer](https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html) [0.1 puntos]\n",
    "3. Crear y aplicar un `ColumnTransformer` actualizado con los pipelines `categoric_transformations` y `numeric_transformations` a `df_retail`, creando un dataframe llamado `df_mean_imputer`. [0.1 puntos]\n",
    "4. Comparar los resultados de `explore_data` en `df_mean_imputer` y `df_iqr`. ¬øQu√© diferencias observa en la distribuci√≥n de los datos? [0.2 puntos]\n",
    "5. Cambiar el imputer de `numeric_transformations` por [KNNImputer](https://scikit-learn.org/stable/modules/generated/sklearn.impute.KNNImputer.html) y definir un nuevo dataframe llamado `df_knn_imputer`, aplicando el nuevo ColumnTransformer a `df_retail`. En caso de los tiempos de ejecuci√≥n sean altos puede probar a reducir el par√°metro `n_neighbors`. [0.1 puntos]\n",
    "6. Comparar los resultados de `explore_data` en `df_knn_imputer` y `df_iqr`. ¬øQu√© diferencias observa en la distribuci√≥n de los datos? [0.2 puntos]\n",
    "7. Comparar los resultados de `explore_data` en `df_knn_imputer` y `df_mean_imputer`. ¬øCu√°l m√©todo de imputaci√≥n es mejor? Deje el m√©todo escogido en el ColumnTransformer. [0.2 puntos]\n",
    "\n",
    "**Nota: Fije el par√°metro verbose_feature_names_out en `False` y utilice el m√©todo set_output con transformaci√≥n `pandas` en cada ColumnTransformer para obtener como salida un dataframe.**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "ACVUdZZxuo4o"
   },
   "source": [
    "**Respuesta:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x8jgag-EYFai"
   },
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "categoric_transformations = Pipeline([\n",
    "    (\"mode_imputer\", SimpleImputer(strategy=\"most_frequent\"))\n",
    "])\n",
    "\n",
    "numeric_transformations = Pipeline([\n",
    "    (\"iqr_filter\", IQR(lam=1.5)),\n",
    "    (\"mean_imputer\", SimpleImputer(strategy=\"mean\"))\n",
    "])\n",
    "\n",
    "numerical_columns = [\"Price\", \"Quantity\"]\n",
    "categorical_columns = [\"Invoice\", \"StockCode\", \"Description\", \"InvoiceDate\", \"Customer ID\", \"Country\"]\n",
    "\n",
    "column_transformer_mean = ColumnTransformer([\n",
    "    (\"numerical\", numeric_transformations, numerical_columns),\n",
    "    (\"categorical\", categoric_transformations, categorical_columns)\n",
    "], verbose_feature_names_out=False)\n",
    "\n",
    "column_transformer_mean.set_output(transform=\"pandas\")\n",
    "\n",
    "df_mean_imputer = column_transformer_mean.fit_transform(df_retail)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explore_data(df_mean_imputer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "numeric_transformations_knn = Pipeline([\n",
    "    (\"iqr_filter\", IQR(lam=1.5)),\n",
    "    (\"knn_imputer\", KNNImputer(n_neighbors=5))  \n",
    "])\n",
    "\n",
    "column_transformer_knn = ColumnTransformer([\n",
    "    (\"numerical\", numeric_transformations_knn, numerical_columns),\n",
    "    (\"categorical\", categoric_transformations, categorical_columns)\n",
    "], verbose_feature_names_out=False)\n",
    "\n",
    "column_transformer_knn.set_output(transform=\"pandas\")\n",
    "\n",
    "df_knn_imputer = column_transformer_knn.fit_transform(df_retail)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explore_data(df_knn_imputer)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "9PfBHpAsvSuD"
   },
   "source": [
    "4) Al comparar df_mean_imputer y df_iqr, la variable Price mantiene una distribuci√≥n muy similar, lo cual indica que la imputaci√≥n por media no afect√≥ significativamente esa variable. Sin embargo, en Quantity se observan aumentos notorios en valores frecuentes como 12, 6 y 2, lo que sugiere que el uso de la media gener√≥ concentraciones artificiales en ciertos valores. En resumen, la imputaci√≥n por media puede mantener la forma general en algunas variables, pero distorsionar otras.\n",
    "\n",
    "6) Al observar df_knn_imputer en comparaci√≥n con df_iqr, se aprecia que la distribuci√≥n en ambas variables es muy parecida a la original. Aunque algunos valores comunes como 12 siguen siendo frecuentes, no se exageran como con la media. La distribuci√≥n es m√°s suave y natural, especialmente en Quantity, lo que refleja que el m√©todo KNN imputa valores de manera m√°s contextual y representativa.\n",
    "\n",
    "7) Comparando df_knn_imputer y df_mean_imputer, se concluye que KNN es superior, especialmente en la variable Quantity. Mientras la media tiende a concentrar los datos en valores frecuentes, KNN mantiene una distribuci√≥n m√°s equilibrada, lo que es beneficioso para evitar sesgos. Por tanto, se escoge KNNImputer como m√©todo de imputaci√≥n final, al preservar mejor la estructura estad√≠stica sin introducir distorsiones notables.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "buuUiW-9YYZ3"
   },
   "source": [
    "### 4. Creaci√≥n de nuevas features [2.0 puntos]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "RQSuoL5mubnA"
   },
   "source": [
    "<p align=\"center\">\n",
    "  <img width=250 src=\"https://miro.medium.com/max/1000/1*JtTWgAcfVTWV8OTjT47Atg.jpeg\">\n",
    "</p>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "4-yHP5oIvzFS"
   },
   "source": [
    "#### 4.1 Definicion de LRMFP [1.0 puntos]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "qe0V2CnZY8Bc"
   },
   "source": [
    "Dado que Mr. Lepin est√° interesado en obtener nuevos atributos relevantes para su negocio, su equipo de expertos sugiere la construcci√≥n de variables **LRMFP**, las que se construyen en base a las siguientes definiciones:\n",
    "\n",
    "- **Length (L)**: Intervalo de tiempo, en d√≠as, entre la primera y la √∫ltima visita del cliente. Mientras mas grande sea el valor, mas fiel es el cliente.\n",
    "\n",
    "- **Recency (R)**: Indica hace cuanto tiempo el cliente realizo su ultima compra. Notar que para este caso, mientras mas grande es el valor, menos interes posee el usuario para repetir una compra en uno de los locales. **Considere \"hoy\" como la fecha mas reciente del dataset**.\n",
    "\n",
    "- **Monetary (M)**: El t√©rmino \"monetario\" se refiere a la cantidad media de dinero gastada por cada visita del cliente durante el per√≠odo de observaci√≥n y refleja la contribuci√≥n del cliente a los ingresos de la empresa.\n",
    "\n",
    "- **Frequency (F)**: Se refiere al n√∫mero total de visitas del cliente durante el periodo de observaci√≥n. Cuanto mayor sea la frecuencia, mayor ser√° la fidelidad del cliente.\n",
    "\n",
    "- **Periodicity (P)**: Representa si los clientes visitan las tiendas con regularidad.\n",
    "\n",
    "$$Periodicity(n)=std(IVT_1, ..., IVT_n)$$\n",
    "\n",
    "Donde $IVT$ denota el tiempo entre visitas y n representa el n√∫mero de valores de tiempo entre visitas de un cliente.\n",
    "\n",
    "\n",
    "$$IVT_i=date\\_diff(t_{i+1},t)$$\n",
    "\n",
    "En base a las definiciones se√±aladas, dise√±e una funci√≥n que permita obtener las caracter√≠sticas **LRMFP** recibiendo un DataFrame como entrada. Para esto, no estar√° permitido el uso de iteradores, utilice todas las herramientas que les ofrece `pandas` para realizar esto.\n",
    "\n",
    "Una referencia que le puede ser √∫til es el [documento original](https://www.researchgate.net/publication/315979555_LRFMP_model_for_customer_segmentation_in_the_grocery_retail_industry_a_case_study) en donde se propone este m√©todo."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "bee8d549c7c043a5b0cafae0543afadf",
    "deepnote_cell_height": 212.6666717529297,
    "deepnote_cell_type": "markdown",
    "id": "L7ZwWJxhXMfk",
    "tags": []
   },
   "source": [
    "**<u>Formato</u> del Resultado Esperado:**\n",
    "\n",
    "| Customer ID | Length | Recency | Frequency | Monetary | Periodicity |\n",
    "|------------:|-------:|--------:|----------:|---------:|------------:|\n",
    "|   12346.0   |    294 |      67 |        46 |   -64.68 |        37.0 |\n",
    "|   12347.0   |     37 |       3 |        71 |  1323.32 |         0.0 |\n",
    "|   12349.0   |    327 |      43 |       107 |  2646.99 |        78.0 |\n",
    "|   12352.0   |     16 |      11 |        18 |   343.80 |         0.0 |\n",
    "|   12356.0   |     44 |      16 |        84 |  3562.25 |        12.0 |"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "3c7f8a4a06a44cbd8d50e8a4decf4c71",
    "deepnote_cell_height": 52.26666259765625,
    "deepnote_cell_type": "markdown",
    "id": "6GaQZaMXXMfk",
    "tags": []
   },
   "source": [
    "**Respuesta:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "39a8b98eacdc43a4bdfeaa138b746198",
    "deepnote_cell_height": 83.86666870117188,
    "deepnote_cell_type": "code",
    "id": "VsgqgqsjXMfl",
    "owner_user_id": "8c58f50a-7a08-41a2-952e-38bdb7507048",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_LRMFP_features(df):\n",
    "    df = df.copy()\n",
    "    df[\"InvoiceDate\"] = pd.to_datetime(df[\"InvoiceDate\"])\n",
    "\n",
    "    df[\"Total\"] = df[\"Price\"] * df[\"Quantity\"]\n",
    "\n",
    "    fecha_max = df[\"InvoiceDate\"].max()\n",
    "\n",
    "    agrupado = df.groupby(\"Customer ID\")\n",
    "\n",
    "    length = (agrupado[\"InvoiceDate\"].max() - agrupado[\"InvoiceDate\"].min()).dt.days\n",
    "\n",
    "    recency = (fecha_max - agrupado[\"InvoiceDate\"].max()).dt.days\n",
    "\n",
    "    frequency = agrupado[\"Invoice\"].nunique()\n",
    "\n",
    "    monetary = agrupado[\"Total\"].sum() / frequency\n",
    "\n",
    "    df_sorted = df.sort_values([\"Customer ID\", \"InvoiceDate\"])\n",
    "    df_sorted[\"IVT\"] = df_sorted.groupby(\"Customer ID\")[\"InvoiceDate\"].diff().dt.days\n",
    "    periodicity = df_sorted.groupby(\"Customer ID\")[\"IVT\"].std()\n",
    "\n",
    "    lrmfp = pd.DataFrame({\n",
    "        \"Length\": length,\n",
    "        \"Recency\": recency,\n",
    "        \"Frequency\": frequency,\n",
    "        \"Monetary\": monetary,\n",
    "        \"Periodicity\": periodicity\n",
    "    })\n",
    "\n",
    "    return lrmfp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lrmfp = get_LRMFP_features(df_retail)\n",
    "df_lrmfp.head()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "1ddL8wThv36t"
   },
   "source": [
    "#### 4.2 Agregando las custom features [1.0 puntos]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "ehLWiQzjwDm-"
   },
   "source": [
    "Ahora, usted decide agregar al pipeline las nuevas variables creadas, para lo cual realiza las siguientes tareas:\n",
    "\n",
    "1. Cree un nuevo pipeline llamado `retail_pipeline` que encapsule el ColumnTransformer y calcule las LRMFP. El primer paso del pipeline ll√°melo  `col_tranformer` y el segundo paso ll√°melo `custom_features`, incorpora las nuevas variables al dataframe. Hint: les puede ser √∫til investigar [este](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.FunctionTransformer.html) m√©todo. [0.1 puntos]\n",
    "2. Aplicar el pipeline actualizado a los datos proporcionados por Mr. Cheems, creando un nuevo dataframe llamado `df_custom`. [0.1 puntos]\n",
    "3. Explorar la distribuci√≥n de las nuevas variables con `explore_data` y comentar brevemente (2-3 l√≠neas) caracter√≠sticas de cada custom feature. [0.5 puntos]\n",
    "5. Entregar un insight para el negocio en base a las nuevas variables. [0.3 puntos]\n",
    "\n",
    "**Nota:** Recuerde fijar el par√°metro `verbose_feature_names_out` en `False` e incorporar el m√©todo `set_output` para obtener una salida en formato dataframe del ColumnTransformer."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "HVCGxPgtwFsk"
   },
   "source": [
    "**Respuesta**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JxILi3w0wE9Q"
   },
   "outputs": [],
   "source": [
    "\n",
    "def add_lrmfp_features(df):\n",
    "    lrmfp = get_LRMFP_features(df)\n",
    "    df = df.copy()\n",
    "    df = df.drop_duplicates(subset=\"Customer ID\")  \n",
    "    df = df.merge(lrmfp, on=\"Customer ID\", how=\"left\")\n",
    "    return df\n",
    "\n",
    "custom_features = FunctionTransformer(add_lrmfp_features, validate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "retail_pipeline = Pipeline([\n",
    "    (\"col_transformer\", column_transformer_knn), \n",
    "    (\"custom_features\", custom_features)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_custom = retail_pipeline.fit_transform(df_retail)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_custom.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explore_data(df_custom, columns=[\"Length\", \"Recency\", \"Monetary\", \"Frequency\", \"Periodicity\"])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al explorar la distribuci√≥n de las nuevas variables LRMFP, se observa que:\n",
    "\n",
    "\n",
    "- Length muestra una distribuci√≥n con una gran cantidad de clientes con poca antig√ºedad y otro grupo con m√°s de 250 d√≠as. Esto sugiere dos tipos de clientes: recientes y recurrentes.\n",
    "\n",
    "\n",
    "- Recency tiene una distribuci√≥n fuertemente sesgada a la derecha, indicando que la mayor√≠a de los clientes han comprado recientemente. Es √∫til para identificar clientes activos frente a inactivos.\n",
    "\n",
    "\n",
    "- Monetary presenta una distribuci√≥n sesgada a la derecha, lo cual indica que la mayor√≠a de los clientes hacen compras peque√±as, pero unos pocos realizan compras muy elevadas.\n",
    "\n",
    "\n",
    "- Frequency  muestra una distribuci√≥n muy concentrada en valores bajos (especialmente en 1), lo que indica que la mayor√≠a de los clientes compra pocas veces, y muy pocos son compradores frecuentes.\n",
    "\n",
    "\n",
    "- Periodicity tambi√©n est√° sesgada a la derecha, lo cual significa que los clientes que repiten compras tienden a hacerlo con bastante separaci√≥n temporal. La mediana parece estar por debajo de 20 d√≠as.\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "insight:\n",
    "El an√°lisis de las variables LRMFP indica que la mayor√≠a de los clientes son compradores de bajo valor y con poca frecuencia, pero hay un grupo m√°s reducido de clientes fieles y de alto gasto. Esto sugiere que el negocio puede beneficiarse significativamente si enfoca campa√±as de retenci√≥n y fidelizaci√≥n hacia estos clientes frecuentes y de alto Monetary, ya que representan un alto valor con menor esfuerzo de adquisici√≥n.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "qOV0y-e_lS39"
   },
   "source": [
    "### 5. MinMax Scaler [1.0 puntos]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "T55ZgReXvjGe"
   },
   "source": [
    "<p align=\"center\">\n",
    "  <img width=300 src=\"https://i.imgflip.com/1fsprn.jpg\">\n",
    "</p>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "4dk2R1kvuu-e"
   },
   "source": [
    "#### 5.1 Definici√≥n del Column Transformer [0.5 puntos]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "94c48775ecb4496d970fbd920f65c126",
    "deepnote_cell_height": 268.70001220703125,
    "deepnote_cell_type": "markdown",
    "id": "iWsfp1dKXMfo",
    "tags": []
   },
   "source": [
    "Construya una clase llamada `MinMax()` para realizar una transformaci√≥n de cada una de las columnas de un DataFrame utilizando `ColumnTransformer()`. Recuerde  usar `BaseEstimator` y `TransformerMixin`.\n",
    "\n",
    "\n",
    " Para esto considere que Min-Max escaler queda dada por la ecuaci√≥n:\n",
    "\n",
    "$$MinMax = \\dfrac{x-min(x)}{max(x) - min(x)}$$\n",
    "\n",
    "\n",
    "Consulte el siguiente [link](https://sklearn-template.readthedocs.io/en/latest/user_guide.html#transformer) si tiene dudas sobre la creaci√≥n de custom transformers."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "c087d1fa8aa94d7485fe1292bf628660",
    "deepnote_cell_height": 52.26666259765625,
    "deepnote_cell_type": "markdown",
    "id": "MUOLTWPDXMfo",
    "tags": []
   },
   "source": [
    "**Respuesta:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "07cb4dcf097c4c6baabb9ae2bda25caf",
    "deepnote_cell_height": 83.86666870117188,
    "deepnote_cell_type": "code",
    "id": "g15ZMCs-XMfo",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MinMax(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.min = None\n",
    "        self.max = None\n",
    "\n",
    "    def fit(self,X):\n",
    "        X = pd.DataFrame(X)\n",
    "        self.min = X.min()\n",
    "        self.max = X.max()\n",
    "        return self\n",
    "\n",
    "    def transform(self,X):\n",
    "        X = pd.DataFrame(X)\n",
    "        return (X - self.min) / (self.max - self.min)\n",
    "\n",
    "    def set_output(self,transform='default'):\n",
    "        #No modificar este m√©todo\n",
    "        return self\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "RySqWq1Muzp8"
   },
   "source": [
    "#### 5.2 Incorporando MinMax al pipeline [0.5 puntos]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "zmIqjkgDwRsV"
   },
   "source": [
    "Ahora, usted decide agregar el escalamiento al pipeline, para lo que decide seguir los siguientes pasos:\n",
    "\n",
    "- Agregar el paso `minmax` al pipeline `numeric_transformations`, haciendo uso de la clase creada. [0.1 puntos]\n",
    "- Defina el dataframe `df_minmax` aplicando el ColumnTransformer actualizado a los datos proporcionados por Mr. Cheems. [0.1 puntos]\n",
    "- Usar `explore_data` en `df_retail` y en `df_minmax`. [0.1 puntos]\n",
    "- Reportar los cambios observados en la distribuci√≥n de las variables.  [0.2 puntos]\n",
    "\n",
    "**Nota:** Recuerde fijar el par√°metro `verbose_feature_names_out` en `False` e incorporar el m√©todo `set_output` para obtener una salida en formato dataframe del ColumnTransformer."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "a480355952a34b6cb7e72afa764091d6",
    "deepnote_cell_height": 52.26666259765625,
    "deepnote_cell_type": "markdown",
    "id": "lL2_CyAGXMfp",
    "tags": []
   },
   "source": [
    "**Respuesta:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "1889976b7a4c40c7825752979b577567",
    "deepnote_cell_height": 65.86666870117188,
    "deepnote_cell_type": "code",
    "id": "NmApXgB8XMfp",
    "tags": []
   },
   "outputs": [],
   "source": [
    "numeric_transformations = Pipeline([\n",
    "    (\"iqr_filter\", IQR(lam=1.5)),\n",
    "    (\"imputer\", SimpleImputer(strategy=\"mean\")),  \n",
    "    (\"minmax\", MinMax())\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_transformer_minmax = ColumnTransformer([\n",
    "    (\"numerical\", numeric_transformations, numerical_columns),\n",
    "    (\"categorical\", categoric_transformations, categorical_columns)\n",
    "], verbose_feature_names_out=False)\n",
    "\n",
    "column_transformer_minmax.set_output(transform=\"pandas\")\n",
    "\n",
    "df_minmax = column_transformer_minmax.fit_transform(df_retail)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Distribuci√≥n en datos originales:\")\n",
    "explore_data(df_retail, columns=[\"Price\", \"Quantity\"])\n",
    "\n",
    "print(\"Distribuci√≥n en datos escalados:\")\n",
    "explore_data(df_minmax, columns=[\"Price\", \"Quantity\"])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luego de aplicar la transformaci√≥n MinMax, se observan cambios importantes en la escala de las variables num√©ricas. En el caso de Price, pas√≥ de tener valores con una distribuci√≥n altamente sesgada a la izquierda (con algunos valores extremos por sobre los 10.000) a estar contenida entre 0 y 1, donde la mayor√≠a de los valores se concentra bajo 0.8. Esto evita que los valores extremos dominen la escala. En cuanto a Quantity, tambi√©n se observaba una fuerte concentraci√≥n en valores bajos (con outliers sobre 18.000). Tras la normalizaci√≥n, estos valores fueron mapeados al intervalo [0, 1], permitiendo una comparaci√≥n m√°s justa entre registros y eliminando el sesgo de escala. Ambos histogramas normalizados presentan una distribuci√≥n m√°s compacta y comparable entre observaciones."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "iWXlAO8-wfNt"
   },
   "source": [
    "### 6. Pregunta te√≥rica [0.5 puntos]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "JvsFRwpVtMh_"
   },
   "source": [
    "<p align=\"center\">\n",
    "  <img width=300 src=\"https://file.coinexstatic.com/2023-09-19/166BAC031F222E5910954E7D7D0BC844.png\">\n",
    "</p>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "Ou7lQIAHwiZv"
   },
   "source": [
    "Finalmente, expl√≠quele a Mr. Cheems porqu√© es √∫til la creaci√≥n de pipelines al momento de hacer Feature Engineering en Machine Learning."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "29QJyzOCwjdD"
   },
   "source": [
    "**Respuesta:**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "YMDYYL1stUVO"
   },
   "source": [
    "La creaci√≥n de pipelines en el proceso de Feature Engineering es √∫til porque permite automatizar, organizar y estandarizar todas las etapas de preprocesamiento de datos de manera estructurada. Con un pipeline, cada paso (como limpieza, imputaci√≥n, escalamiento y generaci√≥n de nuevas variables) se aplica secuencialmente y de forma reproducible, lo que reduce errores y asegura que los mismos procedimientos se apliquen tanto en los datos de entrenamiento como en datos futuros. En resumen, los pipelines hacen que el procesamiento de datos sea m√°s limpio, confiable y escalable."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "94721075d5ff44bd83601c871797ae2a",
    "deepnote_cell_height": 514.4666748046875,
    "deepnote_cell_type": "markdown",
    "id": "Rg4ZMq8ezAH6"
   },
   "source": [
    "# Conclusi√≥n\n",
    "Eso ha sido todo para el lab de hoy, recuerden que el laboratorio tiene un plazo de entrega de una semana. Cualquier duda del laboratorio, no duden en contactarnos por el foro de U-cursos o por correo.\n",
    "\n",
    "![Gracias Totales!](https://i.pinimg.com/originals/65/ae/27/65ae270df87c3c4adcea997e48f60852.gif \"bruno\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "7e31a91f8cb744cabd0ed0287ac5257e",
    "deepnote_cell_height": 171.28334045410156,
    "deepnote_cell_type": "markdown",
    "id": "wCL1lACBzAH7"
   },
   "source": [
    "<br>\n",
    "<center>\n",
    "<img src=\"https://i.kym-cdn.com/photos/images/original/001/194/195/b18.png\" width=100 height=50 />\n",
    "</center>\n",
    "<br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "created_in_deepnote_cell": true,
    "deepnote_cell_type": "markdown",
    "id": "ALHqwrAFXMgD"
   },
   "source": [
    "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=87110296-876e-426f-b91d-aaf681223468' target=\"_blank\">\n",
    "<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\n",
    "Created in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "wrG4gYabzAHs",
    "MhISwri4zAHy",
    "QDwIXTh7bK_A",
    "Q6nm_0uWvrFv",
    "F4ZY_N0Ad1GP",
    "ECqH4t-Jvj05",
    "Pse94ohOm1um",
    "MF5s4dqMYCbJ",
    "buuUiW-9YYZ3",
    "1ddL8wThv36t",
    "qOV0y-e_lS39",
    "4dk2R1kvuu-e",
    "RySqWq1Muzp8",
    "iWXlAO8-wfNt"
   ],
   "provenance": []
  },
  "deepnote_execution_queue": [],
  "deepnote_notebook_id": "33c253a4f84d40a091bd5023e95abb64",
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
